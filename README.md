# K8s ML Training Lab - Local Infrastructure Setup

ğŸš€ **Complete infrastructure setup for Kubernetes-based ML workloads with automated local cluster provisioning and operator management**

A production-ready, infrastructure-first solution that gets you from zero to a fully configured K8s ML environment in 15 minutes.

## ğŸ¯ What This Project Provides

âœ… **Automated Cluster Setup**: Kind clusters optimized for ML workloads on laptops/workstations  
âœ… **Operator Management**: Automated Kubeflow Training Operator installation and configuration  
âœ… **Infrastructure as Code**: Declarative cluster configurations and resource management  
âœ… **Multi-Backend Support**: Flexible infrastructure supporting various ML frameworks  
âœ… **Resource Optimization**: Memory and CPU configurations tuned for local development  
âœ… **Fault-Tolerant Setup**: Robust infrastructure with multiple fallback mechanisms  
âœ… **Ready-to-Use Examples**: Pre-configured distributed PyTorch training as proof-of-concept  

## ğŸ“ Project Structure

```
k8s-ml-lab/
â”œâ”€â”€ bin/                                # Executable scripts
â”‚   â””â”€â”€ setup.sh                       # Infrastructure automation script
â”œâ”€â”€ configs/                            # Infrastructure configurations
â”‚   â”œâ”€â”€ kind-cluster-config.yaml        # Kind cluster specification
â”‚   â”œâ”€â”€ pytorch-distributed-job.yaml    # Sample workload configuration
â”‚   â””â”€â”€ pytorch-test-job.yaml           # Test workload configuration
â”œâ”€â”€ scripts/                            # Sample ML workloads
â”‚   â”œâ”€â”€ mnist_training.py               # Unified training script (single + distributed)
â”‚   â””â”€â”€ test_mnist_model.py             # Model inference example
â”œâ”€â”€ input/                              # Input datasets (auto-populated)
â”œâ”€â”€ output/                             # Training outputs (auto-created)
â”œâ”€â”€ examples/                           # Infrastructure examples and guides
â”‚   â”œâ”€â”€ README.md                       # ğŸ“š Comprehensive documentation
â”‚   â”œâ”€â”€ 01-complete-workflow/           # Complete infrastructure + workload demo
â”‚   â”œâ”€â”€ 02-existing-cluster/            # Existing cluster integration
â”‚   â”œâ”€â”€ 03-custom-dataset/              # Custom workload configurations
â”‚   â”œâ”€â”€ 04-gpu-training/                # GPU-enabled cluster setup
â”‚   â”œâ”€â”€ 05-debugging/                   # Infrastructure debugging guide
â”‚   â””â”€â”€ 06-common-issues/               # Infrastructure troubleshooting
â”œâ”€â”€ Makefile                           # Infrastructure automation commands
â””â”€â”€ README.md                          # This file
```

## ğŸš€ Quick Infrastructure Setup (15 minutes)

### Prerequisites
- **macOS 11+** or **Linux** (Ubuntu, Fedora, Debian, etc.)
- **8GB+ RAM** (16GB recommended)
- **4+ CPU cores**, **10GB free disk space**
- **Docker or Podman** (container runtime)

### 1. Infrastructure Setup
```bash
# Clone repository
git clone https://github.com/<your-username>/k8s-ml-lab.git
cd k8s-ml-lab

# Automated infrastructure setup (recommended)
make setup                    # Complete setup: cluster + operators + training environment

# Alternative: Configure existing cluster
make use-existing             # For EKS, GKE, AKS, minikube, etc.
```

### 2. Verify Infrastructure
```bash
# Comprehensive system verification (recommended first step)
make verify-system       # Checks system requirements + all dependencies

# Check cluster status
make status

# Submit test workload
make submit-job

# View workload logs
make logs

# OR: Run complete end-to-end workflow
make run-e2e-workflow    # Runs training + inference + testing automatically
```

### 3. Test ML Workload
```bash
# Test the sample distributed training workload
python scripts/test_mnist_model.py

# OR: Use make command for easier testing
make inference                                    # Test with built-in images
TEST_IMAGE=path/to/digit.png make inference       # Test single custom image
TEST_IMAGES_DIR=my_digits/ make inference         # Test directory of images
```

## ğŸ“Š Expected Infrastructure Results

```
SUCCESS: Kind cluster 'ml-training-cluster' created
SUCCESS: Kubeflow Training Operator installed
SUCCESS: gloo backend initialized - Rank 0, World size 2
Rank 0: Using pre-downloaded MNIST dataset (60000 train, 10000 test)
âœ… Infrastructure ready for ML workloads!
âœ… Sample workload completed successfully!
```

**Generated Infrastructure:**
- Kubernetes cluster with ML-optimized configuration
- Kubeflow Training Operator for distributed workloads
- Persistent storage for datasets and models
- Network policies and resource quotas
- Sample workload demonstrating capabilities

## ğŸ”„ End-to-End Workflow

The `make run-e2e-workflow` command runs the complete end-to-end workflow automation:

1. **Training Phase**: Submits distributed PyTorch training job
2. **Monitoring Phase**: Tracks job progress and collects logs
3. **Inference Phase**: Tests trained model with sample images
4. **Results Phase**: Generates performance reports and saves outputs

**What it does:**
- Creates and submits PyTorch distributed training job
- Monitors job completion and downloads training logs
- Extracts trained model from completed pods
- Runs inference tests on sample handwritten digit images
- Generates training metrics and accuracy reports
- Saves all outputs to `output/latest/` directory

**Example output:**
```
âœ… Training job submitted and completed
âœ… Model extracted: output/latest/trained-model.pth
âœ… Inference tests passed: 8/10 correct predictions
âœ… Training metrics saved: output/latest/training_metadata.txt
```

## ğŸ” System Verification

The `make verify-system` command performs comprehensive verification of your system readiness:

**System Requirements Check:**
- Memory: minimum 8GB, recommended 16GB
- CPU: minimum 4 cores
- Disk space: minimum 10GB free
- Operating system and architecture detection

**Dependencies Verification:**
- Container runtime: Docker/Podman installation and status
- Python: version and availability
- kubectl: Kubernetes CLI installation and version
- kind: Kubernetes in Docker installation
- Python packages: PyTorch, torchvision, requests, PyYAML

**Infrastructure Status:**
- Kubernetes cluster accessibility
- Kubeflow Training Operator installation status
- Overall readiness assessment

**Example output:**
```
âœ… Memory: 16GB (sufficient)
âœ… CPU cores: 8 (sufficient)
âœ… Disk space: 45GB free (sufficient)
âœ… Docker: installed and running
âœ… Python: 3.11.5
âœ… kubectl: v1.28.0
âœ… kind: v0.20.0
âœ… Python dependencies: PyTorch, torchvision, requests, PyYAML installed
âœ… Kubernetes cluster: accessible
âœ… Kubeflow Training Operator: installed
âœ… System verification completed - all dependencies are ready!
```

**Use this command:**
- Before starting any setup to identify missing dependencies
- After setup to confirm everything is working
- When troubleshooting issues
- As part of CI/CD pipeline validation

## ğŸ“š Documentation

**ğŸ‘‰ [Complete Documentation](examples/README.md)** - Detailed infrastructure guides, architecture, troubleshooting, and advanced configurations

### Quick Links
- **[Setup Guide](examples/README.md#setup-guide)** - Detailed installation and configuration
- **[Architecture](examples/README.md#architecture)** - Infrastructure components and design
- **[Complete Workflow](examples/01-complete-workflow/)** - End-to-end infrastructure + workload demo
- **[Existing Clusters](examples/02-existing-cluster/)** - Integrate with EKS, GKE, AKS, etc.
- **[Custom Workloads](examples/03-custom-dataset/)** - Configure your own ML workloads
- **[GPU Infrastructure](examples/04-gpu-training/)** - GPU-enabled cluster setup
- **[Debugging](examples/05-debugging/)** - Infrastructure debugging techniques
- **[Troubleshooting](examples/06-common-issues/)** - Common infrastructure problems and solutions

## ğŸ”§ Common Infrastructure Commands

```bash
# Infrastructure Management
make setup               # Complete infrastructure setup (cluster + dependencies + training env)
make verify-system       # Comprehensive system and dependency verification
make use-existing        # Use existing cluster (skip cluster creation)

# Training & Workflows
make submit-job          # Submit PyTorch distributed training job
make run-e2e-workflow    # Run complete end-to-end workflow (training + inference + results)
make inference           # Run model inference on test images (TEST_IMAGE=path or TEST_IMAGES_DIR=path)
make status              # Show job status, pods, and recent events
make logs                # View logs from master pod (real-time)
make restart             # Restart training job (delete + submit)

# Debugging & Monitoring
make debug               # Show comprehensive debugging information

# Cleanup
make cleanup             # Clean up jobs and resources (keep cluster)
make cleanup-all         # Delete entire Kind cluster and all resources

# Aliases (for compatibility)
make check-requirements  # Alias for verify-system
make install-operator    # Install Kubeflow training operator (standalone)
```

## ğŸ¨ Quick Infrastructure Customization

**Scale Infrastructure:**
```yaml
# configs/pytorch-distributed-job.yaml
Worker:
  replicas: 3  # Scale workers from 1 to 3
```

**Custom Cluster Configuration:**
```yaml
# configs/kind-cluster-config.yaml
nodes:
- role: control-plane
- role: worker
- role: worker  # Add more workers
```

**Configure Your Workloads:**
```python
# scripts/mnist_training.py
def load_dataset(rank):
    # Replace with your dataset
    train_dataset = YourDataset('/input/your-data')
    return train_dataset, test_dataset
```

## ğŸ§¹ Cleanup

```bash
# Delete workloads only
make cleanup

# Delete entire infrastructure (Kind cluster)
make cleanup-all
```